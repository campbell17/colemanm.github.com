---
layout: post
date: 2013-09-06 22:32:20 EDT
title: "Terra"
description: "A small project to build a virtual environment to run several geospatial tools on your computer."
categories: blog
---

Inspired by a [couple](https://github.com/zhm/geobox) of [others](https://github.com/tlpinney/osmdevbox), I released a micro project of mine called [Terra](https://github.com/colemanm/terra), to provide a fast way to run several geospatial tools on your computer.

![Terra](/images/post-images/terra-sea.jpg)

Because I work with a variety of GIS datasets all the time, I end up writing lots of scripts and small automation utilities to manipulate, convert, and merge data, in tons of different formats. Working with geo data at scale like this challenges the non-software developer to get comfortable with basic scripting and programming. I've learned a ton in the last couple years about Unix environments, and the community of open source geo tools for working with data in ways that can be pipelined or automated. Fundamental knowledge about bash, Python, or Ruby quickly becomes critical to saving yourself countless hours of repetitive, slow data processing.

<img class="right" src="/images/post-images/gdal_logo.png" alt="GDAL" />The renaissance tool of choice for all sorts of data munging is [GDAL](http://www.gdal.org/), and the resident command line suites of [GDAL](http://www.gdal.org/gdal_utilities.html) and [OGR](http://www.gdal.org/ogr_utilities.html). The GDAL and OGR programs (for raster and vector data, respectively) are super powerful out of the box, once you understand the somewhat obtuse and involved syntax for sending data between datasources, and the myriad translation parameters. But these get extra powerful as multitools for all types of data is when you can read from, and sometimes write to, proprietary data formats like Esri geodatabases, ECW files, MrSID raster images, GeoPDFs, SpatiaLite, and others. Many of these formats, though, require you to build the toolchain from source  on your own, including the associated client libraries, and this process can be a giant pain, particularly for anyone who doesn't want to learn the nuances of `make` and binary building. The primary driver for building Terra was to have a simple, clean, consistent environment with a working base set of geotools. It gives you a prebuilt configuration that you can have up and running in minutes.

Terra uses [Vagrant](http://www.vagrantup.com/), for provisioning virtual machines, and [Chef](http://www.opscode.com/chef/), an automation tool for batching up the setup, and maintaining its configuration. Vagrant is really a wrapper around [VirtualBox](https://www.virtualbox.org/) VMs, and uses base Linux images to give you a clean starting point for each deployment. It's amazing for running dev environments. It's supports both Chef and Puppet, two CM tools for automating installation of software. I used Chef since I like writing Ruby, and created recipes to bootstrap the installs.

This all started because I got sick of setting up custom GDAL builds on desktop systems. Next on the list for this mini project is to provision installs of some other open geo apps, like [TileMill](http://www.mapbox.com/tilemill/) and [CartoDB](http://cartodb.com/), to run locally. Try it out on your computer, all you need is VirtualBox and Vagrant installed, and install is a few simple commands. Check it out [on GitHub](https://github.com/colemanm/terra), follow the [README](https://github.com/colemanm/terra/blob/master/README.md) to get yourself set up, and [post an issue](https://github.com/colemanm/terra/issues) if you'd like to see other functionality included.
